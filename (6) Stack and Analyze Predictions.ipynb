{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a6e692",
   "metadata": {},
   "source": [
    "# The purpose of this code is to gather the predicted images, load them into their original volume and calculate the dice\n",
    "This will save the results as a csv file for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6064f73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "keras-unet init: TF version is >= 2.0.0 - using `tf.keras` instead of `Keras`\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "from skimage.transform import resize\n",
    "from keras_unet.metrics import dice_coef\n",
    "from keras_unet.models import custom_unet\n",
    "from keras_unet.losses import jaccard_distance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import fnmatch\n",
    "import nibabel as nib\n",
    "import shutil\n",
    "import re\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802c37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our keras accuracy metrics - these are common formatting gathered from the github and tensorflow community\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "smooth = 1\n",
    "\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=100): \n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(K.abs(y_true_f * y_pred_f)) \n",
    "    sum_ = K.sum(K.abs(y_true_f) + K.abs(y_pred_f)) \n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth) \n",
    "    return (1 - jac) * smooth \n",
    "\n",
    "def mean_length_error(y_true, y_pred):\n",
    "    y_true_f = K.sum(K.round(K.flatten(y_true)))\n",
    "    y_pred_f = K.sum(K.round(K.flatten(y_pred)))\n",
    "    delta = (y_pred_f - y_true_f)\n",
    "    return K.mean(K.tanh(delta))\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "def np_dice_coef(y_true, y_pred):\n",
    "    tr = y_true.flatten()\n",
    "    pr = y_pred.flatten()\n",
    "    return (2. * np.sum(tr * pr) + smooth) / (np.sum(tr) + np.sum(pr) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb67528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather the images and their path based on a phrase\n",
    "def gather_set(data_path, phrase):\n",
    "    set_of = []\n",
    "    path = data_path + '\\\\'\n",
    "    for f in os.listdir(data_path):\n",
    "      if phrase in f:\n",
    "        set_of.append(f)\n",
    "      else:\n",
    "        continue\n",
    "    set_of = np.array(set_of)\n",
    "\n",
    "    indices = np.array(range(len(set_of))) # we will use this in the next step.\n",
    "\n",
    "    return set_of\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0158d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define where the predictions are, where the tensor will be saved, and what the model used for prediction was\n",
    "\n",
    "filepath_predictions = r\"D:\\EKS-predicted\\KU\\UB-cyst\"\n",
    "filepath_tensors = r\"D:\\EKS-predicted\\Tensors\\KU\\Tensors\"\n",
    "filepath_data = filepath_predictions\n",
    "images = gather_set(filepath_predictions, 'UAB_cyst_35ep')\n",
    "model_name = 'UAB_cyst_35ep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2cf5f4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11399 L\n",
      "13948 R\n",
      "15792 R\n",
      "18341 R\n",
      "18671 L\n",
      "18671 R\n",
      "18745 L\n"
     ]
    }
   ],
   "source": [
    "#this snippet loaded in test patients  - gather a list of images to stack into tensors\n",
    "image_list = []\n",
    "\n",
    "#Emory\n",
    "#phrase_list = ['283935R','290336L','290336R','295106L','295106R']\n",
    "#Mayo\n",
    "#phrase_list = ['380166R','383193L','383193R','385151L','385151R']\n",
    "#UAB\n",
    "#phrase_list = ['457036L','457036R']\n",
    "#KU\n",
    "phrase_list = ['113994L','139486R','157925R','183417R','186714L','186714R','187456L']\n",
    "\n",
    "for i in range(len(phrase_list)):\n",
    "    phrase1 = phrase_list[i][:-2]\n",
    "    phrase2 = phrase_list[i][-1]\n",
    "    print(phrase1, phrase2)\n",
    "    for z in range(len(images)):\n",
    "        name = images[z]\n",
    "        if phrase1 in name:\n",
    "            if phrase2 in name:\n",
    "                image_list.append(name)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cb1b4f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure we have the patient name/number correct from above in a list to pull from\n",
    "id_list = []   \n",
    "for i in range(len(image_list)):\n",
    "    image_name = image_list[i]\n",
    "    unique_id =  image_name[0:17]\n",
    "    id_list.append(unique_id)\n",
    "unique_ids = list(set(id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "597e91c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KU_186714_2_96_R_', 'KU_186714_0_78_R_', 'KU_113994_2_99_L_', 'KU_187456_2_120_L', 'KU_113994_3_108_L', 'KU_157925_0_126_R', 'KU_157925_2_144_R', 'KU_139486_1_99_R_', 'KU_139486_0_126_R', 'KU_157925_1_141_R', 'KU_139486_3_111_R', 'KU_183417_0_129_R', 'KU_186714_2_96_L_', 'KU_113994_0_87_L_', 'KU_186714_3_96_L_', 'KU_186714_3_96_R_', 'KU_183417_3_144_R', 'KU_186714_1_93_R_', 'KU_183417_1_144_R', 'KU_187456_1_120_L', 'KU_186714_1_93_L_', 'KU_187456_3_120_L', 'KU_139486_2_99_R_', 'KU_183417_2_144_R', 'KU_157925_3_144_R', 'KU_187456_0_87_L_', 'KU_186714_0_78_L_', 'KU_113994_1_99_L_']\n"
     ]
    }
   ],
   "source": [
    "print(unique_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f55bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the extra _\n",
    "for i in range(len(unique_ids)):\n",
    "    name = unique_ids[i]\n",
    "    if not name.endswith('_'):\n",
    "        name = name+'_'\n",
    "        unique_ids[i]=name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b25c5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KU_186714_2_96_R_', 'KU_186714_0_78_R_', 'KU_113994_2_99_L_', 'KU_187456_2_120_L_', 'KU_113994_3_108_L_', 'KU_157925_0_126_R_', 'KU_157925_2_144_R_', 'KU_139486_1_99_R_', 'KU_139486_0_126_R_', 'KU_157925_1_141_R_', 'KU_139486_3_111_R_', 'KU_183417_0_129_R_', 'KU_186714_2_96_L_', 'KU_113994_0_87_L_', 'KU_186714_3_96_L_', 'KU_186714_3_96_R_', 'KU_183417_3_144_R_', 'KU_186714_1_93_R_', 'KU_183417_1_144_R_', 'KU_187456_1_120_L_', 'KU_186714_1_93_L_', 'KU_187456_3_120_L_', 'KU_139486_2_99_R_', 'KU_183417_2_144_R_', 'KU_157925_3_144_R_', 'KU_187456_0_87_L_', 'KU_186714_0_78_L_', 'KU_113994_1_99_L_']\n"
     ]
    }
   ],
   "source": [
    "print(unique_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8055505a",
   "metadata": {},
   "source": [
    "## Stack original and predicted images into a tensor for metric calculations\n",
    "\n",
    "Once the image is reconstructed into a 3D tensor we can run accuracy calculations, These only need to be generated once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f538ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will stack our 2D predictions into a 3D tensor for accuracy calculation\n",
    "#this is for the kidney prediction\n",
    "for i in range(len(unique_ids)):\n",
    "    pt_info = unique_ids[i]\n",
    "    pt_num, yr_num, num_slices = re.findall(r'\\d+', pt_info)\n",
    "    tensor = np.zeros((512,512,int(num_slices)))\n",
    "    for x in range(int(num_slices)):\n",
    "        img_name = unique_ids[i]+str(x)+'_K.npy'\n",
    "        image = np.load(filepath_data + '\\\\' + img_name)\n",
    "        img_slice = image\n",
    "        tensor[:,:,x] = img_slice\n",
    "        x = x+1\n",
    "    new_fname = unique_ids[i]+'K.npy'\n",
    "    np.save(os.path.join(filepath_tensors, new_fname), tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5628fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#stack the cyst predictions\n",
    "for i in range(len(unique_ids)):\n",
    "    pt_info = unique_ids[i]\n",
    "    pt_num, yr_num, num_slices = re.findall(r'\\d+', pt_info)\n",
    "    tensor = np.zeros((512,512,int(num_slices)))\n",
    "    for x in range(int(num_slices)):\n",
    "        img_name = unique_ids[i]+str(x)+ '_' + model_name +'_C.npy'\n",
    "        image = np.load(filepath_predictions + '\\\\' + img_name)\n",
    "        img_slice = image[:,:,1]\n",
    "        tensor[:,:,x] = img_slice\n",
    "        x = x+1\n",
    "    new_fname = unique_ids[i]+ model_name +'_Cpred.npy'\n",
    "    np.save(os.path.join(filepath_tensors, new_fname), tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19659c4",
   "metadata": {},
   "source": [
    "# Gather prediction tensors and calculate stat\n",
    "\n",
    "Gather the predictions based on the model name, this will work for both kidneys and cysts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bca7ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define where the tensor is located for the ground truth and which model was used\n",
    "filepath_tensors = r'D:\\EKS-predicted\\Emory\\Tensors'\n",
    "pred_list = gather_set(filepath_tensors, 'INSTITUTION_80-10_35ep')\n",
    "true_list = gather_set(filepath_tensors, '_K.')\n",
    "#print(pred_list)\n",
    "#print(true_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39843b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL_INSTITUTION_80-10_35ep\n",
      "EM_283935_3_114_R_C.npy\n"
     ]
    }
   ],
   "source": [
    "#double check that the items we are looking for make sense and will pull the correct file\n",
    "name =pred_list[3]\n",
    "print(name[-36:-10])\n",
    "test = pred_list[3][:-36]+'C.npy'\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "362ff557",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\EKS-predicted\\\\Emory\\\\Tensors\\\\EM_283935_0_135_R_ALL_INSTITUTIC.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6a69104237fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_tensors\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mpred_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_tensors\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mpred_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m23\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'C.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdice_calc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdice_coef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m23\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TF23\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\EKS-predicted\\\\Emory\\\\Tensors\\\\EM_283935_0_135_R_ALL_INSTITUTIC.npy'"
     ]
    }
   ],
   "source": [
    "#here we use the dice coef we defined above to compare the prediction to the ground truth and save that metric as a dataframe\n",
    "results = []\n",
    "for i in range(len(pred_list)):\n",
    "    prediction = np.load(filepath_tensors + '\\\\'+ pred_list[i])\n",
    "    true = np.load(filepath_tensors + '\\\\'+pred_list[i][:-23]+'C.npy')\n",
    "    dice_calc = dice_coef(true,prediction)\n",
    "    model = pred_list[i][-23:-10]\n",
    "    patient = pred_list[i][:-23]\n",
    "    new_calc = [patient, model, dice_calc.numpy()]\n",
    "    results.append(new_calc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ba0e9e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['KU_113994_0_87_L_', 'UAB_cyst_35ep', 0.7387246021537239], ['KU_113994_1_99_L_', 'UAB_cyst_35ep', 0.7485357092776166], ['KU_113994_2_99_L_', 'UAB_cyst_35ep', 0.7109652364938729], ['KU_113994_3_108_L_', 'UAB_cyst_35ep', 0.7366829706578635], ['KU_139486_0_126_R_', 'UAB_cyst_35ep', 0.8032065047860396], ['KU_139486_1_99_R_', 'UAB_cyst_35ep', 0.7958216703453889], ['KU_139486_2_99_R_', 'UAB_cyst_35ep', 0.8306714329618426], ['KU_139486_3_111_R_', 'UAB_cyst_35ep', 0.7820194874271506], ['KU_157925_0_126_R_', 'UAB_cyst_35ep', 0.7841782541768734], ['KU_157925_1_141_R_', 'UAB_cyst_35ep', 0.6846033186670086], ['KU_157925_2_144_R_', 'UAB_cyst_35ep', 0.6720328638497652], ['KU_157925_3_144_R_', 'UAB_cyst_35ep', 0.6904262656279653], ['KU_183417_0_129_R_', 'UAB_cyst_35ep', 0.7367050832705145], ['KU_183417_1_144_R_', 'UAB_cyst_35ep', 0.6671403010209451], ['KU_183417_2_144_R_', 'UAB_cyst_35ep', 0.7143693730997196], ['KU_183417_3_144_R_', 'UAB_cyst_35ep', 0.6198035052662254], ['KU_186714_0_78_L_', 'UAB_cyst_35ep', 0.8401467261267468], ['KU_186714_0_78_R_', 'UAB_cyst_35ep', 0.7787571976257086], ['KU_186714_1_93_L_', 'UAB_cyst_35ep', 0.8512967703976975], ['KU_186714_1_93_R_', 'UAB_cyst_35ep', 0.7395340136493234], ['KU_186714_2_96_L_', 'UAB_cyst_35ep', 0.8458565937743674], ['KU_186714_2_96_R_', 'UAB_cyst_35ep', 0.7612321377095087], ['KU_186714_3_96_L_', 'UAB_cyst_35ep', 0.8789679743647871], ['KU_186714_3_96_R_', 'UAB_cyst_35ep', 0.817416929382904], ['KU_187456_0_87_L_', 'UAB_cyst_35ep', 0.693955646029478], ['KU_187456_1_120_L_', 'UAB_cyst_35ep', 0.41475703515712475], ['KU_187456_2_120_L_', 'UAB_cyst_35ep', 0.7691941146152425], ['KU_187456_3_120_L_', 'UAB_cyst_35ep', 0.6535697862813656]]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bf89d9",
   "metadata": {},
   "source": [
    "## Save the results into a excel file for manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e273351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the results calculated into an excel file for anlaysis later\n",
    "df = pd.DataFrame(results)\n",
    "df.columns =['Patient Number', 'Model', 'Dice Score']\n",
    "filepath = r\"C:\\Users\\UAB\\data\\excel results\\KU-uab-model-cyst-results.xlsx\"\n",
    "df.to_excel(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4673adae",
   "metadata": {},
   "source": [
    "## Additional Metric Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dcbe0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import medpy\n",
    "import medpy.metric\n",
    "import numpy as np\n",
    "import seg_metrics.seg_metrics as sg\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from skimage import measure, morphology\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import time\n",
    "#import gdown\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "e9d4444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define where the tensor is located for the ground truth and which model was used\n",
    "filepath_tensors = r'D:\\EKS-predicted\\UAB\\Tensors'\n",
    "pred_list = gather_set(filepath_tensors, 'Kansas_cyst_35ep')\n",
    "true_list = gather_set(filepath_tensors, '_C.')\n",
    "#print(pred_list)\n",
    "#print(true_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "38a3813a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kansas_cyst_35ep\n",
      "UB_457036_1_105_R_C.npy\n"
     ]
    }
   ],
   "source": [
    "#double check that the items we are looking for make sense and will pull the correct file\n",
    "name =pred_list[3]\n",
    "print(name[-26:-10])\n",
    "test = pred_list[3][:-26]+'C.npy'\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "bcb2d533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.70s/it]\n",
      "1it [00:01,  1.67s/it]\n",
      "1it [00:01,  1.74s/it]\n",
      "1it [00:01,  1.73s/it]\n",
      "1it [00:01,  1.99s/it]\n",
      "1it [00:01,  1.82s/it]\n",
      "1it [00:01,  1.91s/it]\n",
      "1it [00:01,  1.83s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "labels = [1]\n",
    "spacing = np.array([1,1,1]) \n",
    "df = pd.DataFrame()\n",
    "for i in range(len(pred_list)):\n",
    "    prediction = np.load(filepath_tensors + '\\\\'+ pred_list[i])\n",
    "    true = np.load(filepath_tensors + '\\\\'+pred_list[i][:-26]+'C.npy')\n",
    "    #dice_calc = dice_coef(true,prediction)\n",
    "    metrics = sg.write_metrics(labels=labels,  # exclude background if needed\n",
    "                  gdth_img=true,\n",
    "                  pred_img=prediction,\n",
    "                  #csv_file=csv_file,  # save results to the csv_file \n",
    "                  spacing=spacing,   # assign spacing\n",
    "                  metrics=['hd', 'hd95', 'msd', 'dice', 'jaccard'])\n",
    "    model = pred_list[i][-26:-10]\n",
    "    patient = pred_list[i][:-26]\n",
    "    new_calc = [patient, model]\n",
    "    results.append(new_calc)\n",
    "    met = pd.DataFrame(metrics[0])\n",
    "    label = pd.DataFrame(new_calc).T\n",
    "    #df = df.append(pd.concat([label, met], axis=1))\n",
    "    #new_row = pd.concat([label, met], axis=1)\n",
    "    df = pd.concat([df, pd.DataFrame(metrics[0])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "4192f9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-312-d32b56db7cc7>:5: UserWarning: Pandas requires version '1.4.3' or newer of 'xlsxwriter' (version '1.3.7' currently installed).\n",
      "  df1.to_excel(filepath, index=False)\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(results)\n",
    "data.columns =['Patient Number', 'Model']\n",
    "df1 = pd.concat([data, df], axis=1)\n",
    "filepath = r\"D:\\EKS-predicted\\uab-ku-model-cyst-results.xlsx\"\n",
    "df1.to_excel(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c214e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF23",
   "language": "python",
   "name": "tf23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
